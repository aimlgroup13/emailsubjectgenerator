# -*- coding: utf-8 -*-
"""TestFromHuggingFace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MqDTi08kJ4f0Ar-RUdus4UcbIxa5rM5J
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!

#Install required packages
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
# !pip install rouge_score


import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_NAME =  "sbtraining2020/esubjectgen_llama31_clean"

max_seq_length = 2048
dtype = None
load_in_4bit = True

from transformers import AutoTokenizer, AutoModelForCausalLM
hf_tokenizer = AutoTokenizer.from_pretrained("sbtraining2020/esubjectgen_llama31_clean")
hf_model = AutoModelForCausalLM.from_pretrained("sbtraining2020/esubjectgen_llama31_clean")

alpaca_prompt = """Below is a instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def get_output(email_text):
  #"Generate a summary for the Input section in not more than 50 words"
  inputs = hf_tokenizer(
                        [
                            alpaca_prompt.format(
                                # "Generate a subject for the email body defined in Input section in not more than 50 words", # instruction
                                "Generate a subject for the email text in not more than 10 words",
                                email_text, # input
                                "", # output - leave this blank for generation!
                            )
                        ], return_tensors = "pt").to("cuda")

  from transformers import TextStreamer
  text_streamer = TextStreamer(hf_tokenizer)
  result = hf_model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)
  decoded = hf_tokenizer.batch_decode(result)
  response_text = decoded[0].split("### Response:")[-1].strip().replace('<|end_of_text|>','').replace('<|begin_of_text|>:// ','')
  return response_text

email_text = """Plove is going to go to Dallas.
We are going to leave next Friday when he  gets done (7ish) and go up for the game.
The game is at 11 in the morning,  so we will come home directly after it.
Plove says he has a friend who has a  place in Dallas that we can crash at if we don't want to pay for a hotel.
Do you want to go?
        """
result = get_output(email_text)

print(result)