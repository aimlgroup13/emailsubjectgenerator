{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nLzU4c0D0CaY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxeZL2sRzw3v",
        "outputId": "5d8810b9-c22d-4247-f9d3-770e8793945b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME =  \"sbtraining2020/esubjectgen_llama31_clean\""
      ],
      "metadata": {
        "id": "_lWayDBrz0wI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "ffgZcushE5vb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"sbtraining2020/esubjectgen_llama31_clean\")\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\"sbtraining2020/esubjectgen_llama31_clean\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_AaxMrXFDel",
        "outputId": "1fefa84c-8825-4df5-d7fd-5396b175f5c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is a instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "Q-pyq96l6dXU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(email_text):\n",
        "  #\"Generate a summary for the Input section in not more than 50 words\"\n",
        "  inputs = hf_tokenizer(\n",
        "                        [\n",
        "                            alpaca_prompt.format(\n",
        "                                # \"Generate a subject for the email body defined in Input section in not more than 50 words\", # instruction\n",
        "                                \"Generate a subject for the email text in not more than 10 words\",\n",
        "                                email_text, # input\n",
        "                                \"\", # output - leave this blank for generation!\n",
        "                            )\n",
        "                        ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "  from transformers import TextStreamer\n",
        "  text_streamer = TextStreamer(hf_tokenizer)\n",
        "  result = hf_model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "  decoded = hf_tokenizer.batch_decode(result)\n",
        "  response_text = decoded[0].split(\"### Response:\")[-1].strip().replace('<|end_of_text|>','').replace('<|begin_of_text|>:// ','')\n",
        "  return response_text\n"
      ],
      "metadata": {
        "id": "_L1qMozl5xyV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_text = \"\"\"Plove is going to go to Dallas.\n",
        "We are going to leave next Friday when he  gets done (7ish) and go up for the game.\n",
        "The game is at 11 in the morning,  so we will come home directly after it.\n",
        "Plove says he has a friend who has a  place in Dallas that we can crash at if we don't want to pay for a hotel.\n",
        "Do you want to go?\n",
        "        \"\"\"\n",
        "result = get_output(email_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy85x_ie6IM0",
        "outputId": "f829d51e-02a0-4bc0-8d1c-6a2159e4dadf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is a instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a subject for the email text in not more than 10 words\n",
            "\n",
            "### Input:\n",
            "Plove is going to go to Dallas.\n",
            "We are going to leave next Friday when he  gets done (7ish) and go up for the game.\n",
            "The game is at 11 in the morning,  so we will come home directly after it.\n",
            "Plove says he has a friend who has a  place in Dallas that we can crash at if we don't want to pay for a hotel.\n",
            "Do you want to go?\n",
            "        \n",
            "\n",
            "### Response:\n",
            "Dallas<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z289_q7C6UrB",
        "outputId": "15eac426-3185-4786-ebaa-58503be61a49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dallas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}