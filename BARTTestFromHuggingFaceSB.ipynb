{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "nLzU4c0D0CaY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "VxeZL2sRzw3v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"sbtraining2020/email_bart_1\""
      ],
      "metadata": {
        "id": "_lWayDBrz0wI"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "ffgZcushE5vb"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "hf_tokenizer =BartTokenizer.from_pretrained(MODEL_NAME) # AutoTokenizer.from_pretrained( MODEL_NAME) # \"sbtraining2020/esubjectgen_llama31_clean\")\n",
        "hf_model = BartForConditionalGeneration.from_pretrained(MODEL_NAME) #\"sbtraining2020/esubjectgen_llama31_clean\")"
      ],
      "metadata": {
        "id": "N_AaxMrXFDel"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hf_model.to(device)\n",
        "#hf_model.eval()"
      ],
      "metadata": {
        "id": "Q-pyq96l6dXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d8f0cdb6-48e2-4aa2-9d2f-94dd675aab7c"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltNpL5jZ9bn_"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_email_subject(text, max_length=500):\n",
        "    \"\"\"\n",
        "    Generates a email subject for the given text using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        text (str): Email text to generate subject.\n",
        "        max_length (int): The maximum length of the input text for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated subject for the input text.\n",
        "    \"\"\"\n",
        "    # Encode the input text using the tokenizer. The 'pt' indicates PyTorch tensors.\n",
        "    inputs = hf_tokenizer.encode(text, return_tensors=\"pt\", max_length=max_length, truncation=False)\n",
        "\n",
        "    # Move the encoded text to the same device as the model (e.g., GPU or CPU)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # Generate summary IDs with the model. num_beams controls the beam search width.\n",
        "    # early_stopping is set to False for a thorough search, though it can be set to True for faster results.\n",
        "    subject_ids = hf_model.generate(inputs, max_length=500, num_beams=30, early_stopping=False)\n",
        "\n",
        "    # Decode the generated IDs back to text, skipping special tokens like padding or EOS.\n",
        "    subject = hf_tokenizer.decode(subject_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return the generated subject\n",
        "    return subject"
      ],
      "metadata": {
        "id": "EwNLEeiTIrYz"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_text = \"\"\"Plove is going to go to Dallas.\n",
        "We are going to leave next Friday when he  gets done (7ish) and go up for the game.\n",
        "The game is at 11 in the morning,  so we will come home directly after it.\n",
        "Plove says he has a friend who has a  place in Dallas that we can crash at if we don't want to pay for a hotel.\n",
        "Do you want to go?\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "G2d-qJP1qrR8"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_text = \"\"\"\n",
        "Michelle Here are my very minor comments.\n",
        "However we still need to wait on any additions, based on meeting with SME's today.\n",
        "One concern is the firing of the learner who performs  bad in the final two scenarios.\n",
        "Do we face any copyright issues using the CNN type themes?\n",
        "In addition, I think we need to stay clear of anything that remotely seems like California or anything that really happen with Enron?\n",
        "(i.e.So-cal Waha) In addition, comments on regulatory issues may be a problem (i.e.California Legislature).\n",
        "Sheri  When you read all the scripts together and due to the similar mechanics being taught it appears very repetitious.\n",
        "Thus I do believe we need to maybe use a \"Dateline\" type theme for one, and a \"60 Minute\" type theme for another scenario vice just the CNN type theme.\n",
        "In the last two scenarios can we include a promotion out of the associate program for the stellar performers (i.e.title change to manager)?\n",
        "Cheers Kirk\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ExbmctSeDVmI"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#email_text = \"\"\"The following reports have been waiting for your approval for more than 4 days. Please review. Owner: James W Reitmeyer Report Name: JReitmeyer 10/24/01 Days In Mgr. Queue: 5 \"\"\"\n"
      ],
      "metadata": {
        "id": "lOHCdgIPsBDT"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_email_subject(email_text)"
      ],
      "metadata": {
        "id": "qy85x_ie6IM0"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z289_q7C6UrB"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKUmiN0ws02V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bccbea4-cb64-4e7e-af88-9bd2a23f35e9"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EiBK4oikCB3C",
        "outputId": "4235ef68-f051-4cca-da85-34eb3e047ca4"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Comments from SME's\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0NXqQE3CUPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}